# any-llm Documentation

> Complete documentation for any-llm - A Python library providing a single interface to different llm providers.

This file contains all documentation pages concatenated for easy consumption by AI systems.

---

## index.md

<!-- Source: index.md -->

<p align="center">
  <picture>
    <img src="./images/any-llm-logo.png" width="20%" alt="Project logo"/>
  </picture>
</p>

`any-llm` is a Python library providing a single interface to different llm providers.

### Getting Started

Refer to the [Quickstart](#./quickstart.md) for instructions on installation and usage.

### Parameters

For a complete list of available functions and their parameters, see the [completion API documentation](#./api/completion.md).

### Error Handling

`any-llm` provides custom exceptions to indicate common errors like missing API keys
and parameters that are unsupported by a specific provider.

For more details on exceptions, see the [exceptions API documentation](#./api/exceptions.md).

## For AI Systems

This documentation is available in two AI-friendly formats:

- **[llms.txt](https://mozilla-ai.github.io/any-llm/llms.txt)** - A structured overview with curated links to key documentation sections
- **[llms-full.txt](https://mozilla-ai.github.io/any-llm/llms-full.txt)** - Complete documentation content concatenated into a single file


---

## quickstart.md

<!-- Source: quickstart.md -->

## Quickstart

### Requirements

- Python 3.11 or newer
- API_KEYS to access to whichever LLM you choose to use.

### Installation

#### Direct Usage

In your pip install, include the [supported providers](#./providers.md) that you plan on using, or use the `all` option if you want to install support for all `any-llm` supported providers.

```bash
pip install any-llm-sdk[mistral]  # For Mistral provider
pip install any-llm-sdk[ollama]   # For Ollama provider
# install multiple providers
pip install any-llm-sdk[mistral,ollama]
# or install support for all providers
pip install any-llm-sdk[all]
```

#### Library Integration

If you're integrating `any-llm` into your own library that others will use, you only need to install the base package:

```bash
pip install any-llm-sdk
```

In this scenario, the end users of your library will be responsible for installing the appropriate provider dependencies when they want to use specific providers. `any-llm` is designed so that you'll only encounter exceptions at runtime if you try to use a provider without having the required dependencies installed.

Those exceptions will clearly describe what needs to be installed to resolve the issue.

Make sure you have the appropriate API key environment variable set for your provider. Alternatively,
you could use the `api_key` parameter when making a completion call instead of setting an environment variable.

```bash
export MISTRAL_API_KEY="YOUR_KEY_HERE"  # or OPENAI_API_KEY, etc
```

### Basic Usage

[`completion`][any_llm.completion] and [`acompletion`][any_llm.acompletion] use a unified interface across all providers.

The provider_id key of the model should be specified according the [provider ids supported by any-llm](#./providers.md).
The `model_id` portion is passed directly to the provider internals: to understand what model ids are available for a provider,
you will need to refer to the provider documentation.

```python
from any_llm import completion
import os

# Make sure you have the appropriate environment variable set
assert os.environ.get('MISTRAL_API_KEY')

model = "mistral/mistral-small-latest" # <provider_id>/<model_id>
# Basic completion
response = completion(
    model=model,
    messages=[{"role": "user", "content": "Hello!"}]
)
print(response.choices[0].message.content)
```

In that above script,
updating to use an ollama hosted mistral model (assuming that you have ollama installed and running)
is as easy as updating the model to specify the ollama provider and using
[ollama model syntax for mistral](https://ollama.com/library/mistral-small3.2)!

```python
model="ollama/mistral-small3.2:latest"
```

### Streaming

For the [providers that support streaming](#./providers.md), you can enable it by passing `stream=True`:

```python
output = ""
for chunk in completion(
    model=model,
    messages=[{"role": "user", "content": "Hello!"}],
    stream=True
):
    chunk_content = chunk.choices[0].delta.content or ""
    print(chunk_content)
    output += chunk_content
```

### Embeddings

[`embedding`][any_llm.embedding] and [`aembedding`][any_llm.aembedding] allow you to create vector embeddings from text using the same unified interface across providers.

Not all providers support embeddings - check the [providers documentation](#./providers.md) to see which ones do.

```python
from any_llm import embedding
model = "openai/text-embedding-3-small"
result = embedding(
    model=model,
    inputs="Hello, world!" # can be either string or list of strings
)

# Access the embedding vector
embedding_vector = result.data[0].embedding
print(f"Embedding vector length: {len(embedding_vector)}")
print(f"Tokens used: {result.usage.total_tokens}")
```


---

## providers.md

<!-- Source: providers.md -->

# Supported Providers

`any-llm` supports the below providers. In order to discover information about what models are supported by a provider
as well as what features the provider supports for each model, refer to the provider documentation.

<!-- The below table is auto-generated by the mkdocs build hook. It will display in the generated site -->
<!-- AUTO-GENERATED TABLE START -->
<!-- AUTO-GENERATED TABLE END -->


---

## api/completion.md

<!-- Source: api/completion.md -->

## Completion

::: any_llm.completion
::: any_llm.acompletion


---

## api/embedding.md

<!-- Source: api/embedding.md -->

## Embedding

::: any_llm.embedding
::: any_llm.aembedding


---

## api/exceptions.md

<!-- Source: api/exceptions.md -->

## Exceptions

::: any_llm.exceptions


---

## api/helpers.md

<!-- Source: api/helpers.md -->

## Helpers

::: any_llm.verify_kwargs


---
