{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"any-llm","text":"<p><code>any-llm</code> is a Python library providing a single interface to different llm providers.</p>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python 3.11 or newer</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Install <code>any-llm</code> with the name of the provider that you plan to use</p> <pre><code>pip install any-llm-sdk[mistral,ollama]\n</code></pre> <p>Refer to pyproject.toml for a list of the options available.</p>"},{"location":"#quickstart","title":"Quickstart","text":""},{"location":"#basic-usage","title":"Basic Usage","text":"<p>The primary function is <code>completion</code>, which uses a unified interface across all providers:</p> <pre><code>from any_llm import completion\nimport os\n\n# Make sure you have the appropriate environment variable set\nassert os.environ.get('MISTRAL_API_KEY')\n# Basic completion\nresponse = completion(\n    model=\"mistral/mistral-small-latest\", # &lt;provider&gt;/&lt;model-id&gt;\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"#parameters","title":"Parameters","text":"<p>For a complete list of available parameters, see the completion API documentation.</p>"},{"location":"#error-handling","title":"Error Handling","text":"<p><code>any-llm</code> provides custom exceptions to indicate common errors like missing API keys and parameters that are unsupported by a specific provider.</p> <p>For more details on exceptions, see the exceptions API documentation.</p>"},{"location":"#for-ai-systems","title":"For AI Systems","text":"<p>This documentation is available in two AI-friendly formats:</p> <ul> <li>llms.txt - A structured overview with curated links to key documentation sections</li> <li>llms-full.txt - Complete documentation content concatenated into a single file</li> </ul>"},{"location":"providers/","title":"Supported Providers","text":"<p><code>any-llm</code> supports the following providers:</p> <ul> <li>OpenAI</li> <li>Anthropic</li> <li>Google</li> <li>Mistral</li> <li>Ollama</li> <li>DeepSeek</li> <li>HuggingFace</li> <li>Cohere</li> <li>Cerebras</li> <li>Fireworks</li> <li>Groq</li> <li>AWS Bedrock</li> <li>Azure OpenAI</li> <li>IBM Watsonx</li> <li>Inception Labs</li> <li>Moonshot AI</li> <li>Nebius AI Studio</li> <li>SambaNova</li> <li>Together AI</li> <li>xAI</li> </ul> <p>You can browse the implementations via the providers folder:</p> <p>https://github.com/mozilla-ai/any-llm/tree/main/src/any_llm/providers</p>"},{"location":"api/completion/","title":"Completion","text":""},{"location":"api/completion/#completion","title":"Completion","text":""},{"location":"api/completion/#any_llm.completion","title":"<code>any_llm.completion(model, messages, *, tools=None, tool_choice=None, max_turns=None, temperature=None, top_p=None, max_tokens=None, response_format=None, stream=None, n=None, stop=None, presence_penalty=None, frequency_penalty=None, seed=None, api_key=None, api_base=None, timeout=None, user=None, **kwargs)</code>","text":"<p>Create a chat completion.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier in format 'provider/model' (e.g., 'mistral/mistral-small')</p> required <code>messages</code> <code>list[dict[str, Any]]</code> <p>List of messages for the conversation</p> required <code>tools</code> <code>Optional[List[dict[str, Any]]]</code> <p>List of tools or functions for tool calling</p> <code>None</code> <code>tool_choice</code> <code>Optional[Union[str, dict[str, Any]]]</code> <p>Controls which tools the model can call</p> <code>None</code> <code>max_turns</code> <code>Optional[int]</code> <p>Maximum number of tool execution turns</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>Controls randomness in the response (0.0 to 2.0)</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>Controls diversity via nucleus sampling (0.0 to 1.0)</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>Maximum number of tokens to generate</p> <code>None</code> <code>response_format</code> <code>dict[str, Any] | type[BaseModel] | None</code> <p>Format specification for the response</p> <code>None</code> <code>stream</code> <code>Optional[bool]</code> <p>Whether to stream the response</p> <code>None</code> <code>n</code> <code>Optional[int]</code> <p>Number of completions to generate</p> <code>None</code> <code>stop</code> <code>Optional[Union[str, List[str]]]</code> <p>Stop sequences for generation</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>Penalize new tokens based on presence in text</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Penalize new tokens based on frequency in text</p> <code>None</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducible results</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>Optional[str]</code> <p>Base URL for the provider API</p> <code>None</code> <code>timeout</code> <code>Optional[Union[float, int]]</code> <p>Request timeout in seconds</p> <code>None</code> <code>user</code> <code>Optional[str]</code> <p>Unique identifier for the end user</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>ChatCompletion</code> <p>The completion response from the provider</p> Source code in <code>src/any_llm/api.py</code> <pre><code>def completion(\n    model: str,\n    messages: list[dict[str, Any]],\n    *,\n    # Common parameters with explicit types\n    tools: Optional[List[dict[str, Any]]] = None,\n    tool_choice: Optional[Union[str, dict[str, Any]]] = None,\n    max_turns: Optional[int] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    response_format: dict[str, Any] | type[BaseModel] | None = None,\n    # Generation control\n    stream: Optional[bool] = None,\n    n: Optional[int] = None,\n    stop: Optional[Union[str, List[str]]] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    seed: Optional[int] = None,\n    # Provider configuration\n    api_key: Optional[str] = None,\n    api_base: Optional[str] = None,\n    timeout: Optional[Union[float, int]] = None,\n    user: Optional[str] = None,\n    # Additional provider-specific parameters\n    **kwargs: Any,\n) -&gt; ChatCompletion:\n    \"\"\"Create a chat completion.\n\n    Args:\n        model: Model identifier in format 'provider/model' (e.g., 'mistral/mistral-small')\n        messages: List of messages for the conversation\n        tools: List of tools or functions for tool calling\n        tool_choice: Controls which tools the model can call\n        max_turns: Maximum number of tool execution turns\n        temperature: Controls randomness in the response (0.0 to 2.0)\n        top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n        max_tokens: Maximum number of tokens to generate\n        response_format: Format specification for the response\n        stream: Whether to stream the response\n        n: Number of completions to generate\n        stop: Stop sequences for generation\n        presence_penalty: Penalize new tokens based on presence in text\n        frequency_penalty: Penalize new tokens based on frequency in text\n        seed: Random seed for reproducible results\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        timeout: Request timeout in seconds\n        user: Unique identifier for the end user\n        **kwargs: Additional provider-specific parameters\n\n    Returns:\n        The completion response from the provider\n\n    \"\"\"\n    # Check that correct format is used\n    if \"/\" not in model:\n        msg = f\"Invalid model format. Expected 'provider/model', got '{model}'\"\n        raise ValueError(msg)\n\n    if stream:\n        raise ValueError(\"Streaming is not yet supported\")\n\n    # Extract the provider key from the model identifier, e.g., \"mistral/mistral-small\"\n    provider_key_str, model_name = model.split(\"/\", 1)\n\n    # Validate that neither provider nor model name is empty\n    if not provider_key_str or not model_name:\n        msg = f\"Invalid model format. Expected 'provider/model', got '{model}'\"\n        raise ValueError(msg)\n\n    # Convert string to ProviderName enum and validate\n    provider_key = ProviderFactory.get_provider_enum(provider_key_str)\n\n    # Create provider instance\n    config: dict[str, str] = {}\n    if api_key:\n        config[\"api_key\"] = str(api_key)\n    if api_base:\n        config[\"api_base\"] = str(api_base)\n    api_config = ApiConfig(**config)\n\n    provider = ProviderFactory.create_provider(provider_key, api_config)\n\n    # Build kwargs with explicit parameters\n    completion_kwargs = kwargs.copy()\n    if tools is not None:\n        completion_kwargs[\"tools\"] = tools\n    if tool_choice is not None:\n        completion_kwargs[\"tool_choice\"] = tool_choice\n    if max_turns is not None:\n        completion_kwargs[\"max_turns\"] = max_turns\n    if temperature is not None:\n        completion_kwargs[\"temperature\"] = temperature\n    if top_p is not None:\n        completion_kwargs[\"top_p\"] = top_p\n    if max_tokens is not None:\n        completion_kwargs[\"max_tokens\"] = max_tokens\n    if response_format is not None:\n        completion_kwargs[\"response_format\"] = response_format\n    if stream is not None:\n        completion_kwargs[\"stream\"] = stream\n    if n is not None:\n        completion_kwargs[\"n\"] = n\n    if stop is not None:\n        completion_kwargs[\"stop\"] = stop\n    if presence_penalty is not None:\n        completion_kwargs[\"presence_penalty\"] = presence_penalty\n    if frequency_penalty is not None:\n        completion_kwargs[\"frequency_penalty\"] = frequency_penalty\n    if seed is not None:\n        completion_kwargs[\"seed\"] = seed\n    if timeout is not None:\n        completion_kwargs[\"timeout\"] = timeout\n    if user is not None:\n        completion_kwargs[\"user\"] = user\n\n    return provider.completion(model_name, messages, **completion_kwargs)\n</code></pre>"},{"location":"api/exceptions/","title":"Exceptions","text":""},{"location":"api/exceptions/#exceptions","title":"Exceptions","text":""},{"location":"api/exceptions/#any_llm.exceptions","title":"<code>any_llm.exceptions</code>","text":"<p>Custom exceptions for any-llm package.</p>"},{"location":"api/exceptions/#any_llm.exceptions.MissingApiKeyError","title":"<code>MissingApiKeyError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when an API key is missing or not provided.</p> Source code in <code>src/any_llm/exceptions.py</code> <pre><code>class MissingApiKeyError(Exception):\n    \"\"\"Exception raised when an API key is missing or not provided.\"\"\"\n\n    def __init__(self, provider_name: str, env_var_name: str) -&gt; None:\n        \"\"\"Initialize the exception.\n\n        Args:\n            provider_name: Name of the provider (e.g., \"OpenAI\", \"Google\", \"Mistral\")\n            env_var_name: Name of the environment variable that should contain the API key\n            message: Optional custom message. If not provided, a default message will be used.\n        \"\"\"\n        self.provider_name = provider_name\n        self.env_var_name = env_var_name\n\n        message = (\n            f\"No {provider_name} API key provided. \"\n            f\"Please provide it in the config or set the {env_var_name} environment variable.\"\n        )\n\n        super().__init__(message)\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.MissingApiKeyError.__init__","title":"<code>__init__(provider_name, env_var_name)</code>","text":"<p>Initialize the exception.</p> <p>Parameters:</p> Name Type Description Default <code>provider_name</code> <code>str</code> <p>Name of the provider (e.g., \"OpenAI\", \"Google\", \"Mistral\")</p> required <code>env_var_name</code> <code>str</code> <p>Name of the environment variable that should contain the API key</p> required <code>message</code> <p>Optional custom message. If not provided, a default message will be used.</p> required Source code in <code>src/any_llm/exceptions.py</code> <pre><code>def __init__(self, provider_name: str, env_var_name: str) -&gt; None:\n    \"\"\"Initialize the exception.\n\n    Args:\n        provider_name: Name of the provider (e.g., \"OpenAI\", \"Google\", \"Mistral\")\n        env_var_name: Name of the environment variable that should contain the API key\n        message: Optional custom message. If not provided, a default message will be used.\n    \"\"\"\n    self.provider_name = provider_name\n    self.env_var_name = env_var_name\n\n    message = (\n        f\"No {provider_name} API key provided. \"\n        f\"Please provide it in the config or set the {env_var_name} environment variable.\"\n    )\n\n    super().__init__(message)\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.UnsupportedParameterError","title":"<code>UnsupportedParameterError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when an unsupported parameter is provided.</p> Source code in <code>src/any_llm/exceptions.py</code> <pre><code>class UnsupportedParameterError(Exception):\n    \"\"\"Exception raised when an unsupported parameter is provided.\"\"\"\n\n    def __init__(self, parameter_name: str, provider_name: str) -&gt; None:\n        \"\"\"Initialize the exception.\n\n        Args:\n            parameter_name: Name of the parameter that was provided\n        \"\"\"\n        self.parameter_name = parameter_name\n        self.provider_name = provider_name\n\n        message = f\"'{parameter_name}' is not supported for {provider_name}\"\n\n        super().__init__(message)\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.UnsupportedParameterError.__init__","title":"<code>__init__(parameter_name, provider_name)</code>","text":"<p>Initialize the exception.</p> <p>Parameters:</p> Name Type Description Default <code>parameter_name</code> <code>str</code> <p>Name of the parameter that was provided</p> required Source code in <code>src/any_llm/exceptions.py</code> <pre><code>def __init__(self, parameter_name: str, provider_name: str) -&gt; None:\n    \"\"\"Initialize the exception.\n\n    Args:\n        parameter_name: Name of the parameter that was provided\n    \"\"\"\n    self.parameter_name = parameter_name\n    self.provider_name = provider_name\n\n    message = f\"'{parameter_name}' is not supported for {provider_name}\"\n\n    super().__init__(message)\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.UnsupportedProviderError","title":"<code>UnsupportedProviderError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when an unsupported provider is requested.</p> Source code in <code>src/any_llm/exceptions.py</code> <pre><code>class UnsupportedProviderError(Exception):\n    \"\"\"Exception raised when an unsupported provider is requested.\"\"\"\n\n    def __init__(self, provider_key: str, supported_providers: list[str]) -&gt; None:\n        \"\"\"Initialize the exception.\n\n        Args:\n            provider_key: The provider key that was requested\n            supported_providers: List of supported provider keys\n        \"\"\"\n        self.provider_key = provider_key\n        self.supported_providers = supported_providers\n\n        message = f\"'{provider_key}' is not a supported provider. Supported providers: {', '.join(supported_providers)}\"\n\n        super().__init__(message)\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.UnsupportedProviderError.__init__","title":"<code>__init__(provider_key, supported_providers)</code>","text":"<p>Initialize the exception.</p> <p>Parameters:</p> Name Type Description Default <code>provider_key</code> <code>str</code> <p>The provider key that was requested</p> required <code>supported_providers</code> <code>list[str]</code> <p>List of supported provider keys</p> required Source code in <code>src/any_llm/exceptions.py</code> <pre><code>def __init__(self, provider_key: str, supported_providers: list[str]) -&gt; None:\n    \"\"\"Initialize the exception.\n\n    Args:\n        provider_key: The provider key that was requested\n        supported_providers: List of supported provider keys\n    \"\"\"\n    self.provider_key = provider_key\n    self.supported_providers = supported_providers\n\n    message = f\"'{provider_key}' is not a supported provider. Supported providers: {', '.join(supported_providers)}\"\n\n    super().__init__(message)\n</code></pre>"}]}