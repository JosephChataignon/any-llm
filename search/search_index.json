{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Intro","text":"<p><code>any-llm</code> is a Python library providing a single interface to different llm providers.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Refer to the Quickstart for instructions on installation and usage.</p>"},{"location":"#parameters","title":"Parameters","text":"<p>For a complete list of available functions and their parameters, see the completion, embedding, and responses API documentation.</p>"},{"location":"#error-handling","title":"Error Handling","text":"<p><code>any-llm</code> provides custom exceptions to indicate common errors like missing API keys and parameters that are unsupported by a specific provider.</p> <p>For more details on exceptions, see the exceptions API documentation.</p>"},{"location":"#for-ai-systems","title":"For AI Systems","text":"<p>This documentation is available in two AI-friendly formats:</p> <ul> <li>llms.txt - A structured overview with curated links to key documentation sections</li> <li>llms-full.txt - Complete documentation content concatenated into a single file</li> </ul>"},{"location":"providers/","title":"Supported Providers","text":"<p><code>any-llm</code> supports the below providers. In order to discover information about what models are supported by a provider as well as what features the provider supports for each model, refer to the provider documentation.</p> <p>Legend</p> <ul> <li>Reasoning (Completions): Provider can return reasoning traces alongside the assistant message via the completions and/or streaming endpoints. This does not indicate whether the provider offers separate \"reasoning models\".See this</li> <li>Streaming (Completions): Provider can stream completion results back as an iterator. discussion for more information.</li> <li>Responses API: Provider supports the Responses API variant for text generation.  See this to follow along with our implementation effort.</li> </ul> ID Env Var Source Code Responses Completion Streaming(Completions) Reasoning(Completions) Embedding <code>anthropic</code> ANTHROPIC_API_KEY Source \u274c \u2705 \u2705 \u274c \u274c <code>aws</code> AWS_BEARER_TOKEN_BEDROCK Source \u274c \u2705 \u2705 \u274c \u2705 <code>azure</code> AZURE_API_KEY Source \u274c \u2705 \u2705 \u274c \u2705 <code>cerebras</code> CEREBRAS_API_KEY Source \u274c \u2705 \u2705 \u274c \u274c <code>cohere</code> CO_API_KEY Source \u274c \u2705 \u2705 \u274c \u274c <code>databricks</code> DATABRICKS_TOKEN Source \u274c \u2705 \u2705 \u274c \u2705 <code>deepseek</code> DEEPSEEK_API_KEY Source \u274c \u2705 \u2705 \u274c \u274c <code>fireworks</code> FIREWORKS_API_KEY Source \u2705 \u2705 \u2705 \u274c \u274c <code>google</code> GOOGLE_API_KEY/GEMINI_API_KEY Source \u274c \u2705 \u2705 \u274c \u2705 <code>groq</code> GROQ_API_KEY Source \u2705 \u2705 \u2705 \u2705 \u274c <code>huggingface</code> HF_TOKEN Source \u274c \u2705 \u2705 \u274c \u274c <code>inception</code> INCEPTION_API_KEY Source \u274c \u2705 \u2705 \u274c \u274c <code>llama</code> LLAMA_API_KEY Source \u274c \u2705 \u2705 \u274c \u274c <code>lmstudio</code> LM_STUDIO_API_KEY Source \u274c \u2705 \u2705 \u2705 \u2705 <code>mistral</code> MISTRAL_API_KEY Source \u274c \u2705 \u2705 \u2705 \u2705 <code>moonshot</code> MOONSHOT_API_KEY Source \u274c \u2705 \u2705 \u274c \u274c <code>nebius</code> NEBIUS_API_KEY Source \u274c \u2705 \u2705 \u274c \u2705 <code>ollama</code> None Source \u274c \u2705 \u2705 \u2705 \u2705 <code>openai</code> OPENAI_API_KEY Source \u2705 \u2705 \u2705 \u274c \u2705 <code>openrouter</code> OPENROUTER_API_KEY Source \u274c \u2705 \u2705 \u274c \u274c <code>portkey</code> PORTKEY_API_KEY Source \u274c \u2705 \u2705 \u274c \u274c <code>sambanova</code> SAMBANOVA_API_KEY Source \u274c \u2705 \u2705 \u274c \u2705 <code>together</code> TOGETHER_API_KEY Source \u274c \u2705 \u2705 \u274c \u274c <code>voyage</code> VOYAGE_API_KEY Source \u274c \u274c \u274c \u274c \u2705 <code>watsonx</code> WATSONX_API_KEY Source \u274c \u2705 \u2705 \u274c \u274c <code>xai</code> XAI_API_KEY Source \u274c \u2705 \u2705 \u2705 \u274c"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#quickstart","title":"Quickstart","text":""},{"location":"quickstart/#requirements","title":"Requirements","text":"<ul> <li>Python 3.11 or newer</li> <li>API_KEYS to access to whichever LLM you choose to use.</li> </ul>"},{"location":"quickstart/#installation","title":"Installation","text":""},{"location":"quickstart/#direct-usage","title":"Direct Usage","text":"<p>In your pip install, include the supported providers that you plan on using, or use the <code>all</code> option if you want to install support for all <code>any-llm</code> supported providers.</p> <pre><code>pip install any-llm-sdk[mistral]  # For Mistral provider\npip install any-llm-sdk[ollama]   # For Ollama provider\n# install multiple providers\npip install any-llm-sdk[mistral,ollama]\n# or install support for all providers\npip install any-llm-sdk[all]\n</code></pre>"},{"location":"quickstart/#library-integration","title":"Library Integration","text":"<p>If you're integrating <code>any-llm</code> into your own library that others will use, you only need to install the base package:</p> <pre><code>pip install any-llm-sdk\n</code></pre> <p>In this scenario, the end users of your library will be responsible for installing the appropriate provider dependencies when they want to use specific providers. <code>any-llm</code> is designed so that you'll only encounter exceptions at runtime if you try to use a provider without having the required dependencies installed.</p> <p>Those exceptions will clearly describe what needs to be installed to resolve the issue.</p> <p>Make sure you have the appropriate API key environment variable set for your provider. Alternatively, you could use the <code>api_key</code> parameter when making a completion call instead of setting an environment variable.</p> <pre><code>export MISTRAL_API_KEY=\"YOUR_KEY_HERE\"  # or OPENAI_API_KEY, etc\n</code></pre>"},{"location":"quickstart/#basic-usage","title":"Basic Usage","text":"<p><code>completion</code> and <code>acompletion</code> use a unified interface across all providers.</p> <p>The provider_id key of the model should be specified according the provider ids supported by any-llm. The <code>model_id</code> portion is passed directly to the provider internals: to understand what model ids are available for a provider, you will need to refer to the provider documentation.</p> <pre><code>import os\n\nfrom any_llm import completion\n\n# Make sure you have the appropriate environment variable set\nassert os.environ.get('MISTRAL_API_KEY')\n\nresponse = completion(\n    model=\"mistral/mistral-small-latest\",  # &lt;provider_id&gt;/&lt;model_id&gt;,\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\nprint(response.choices[0].message.content)\n</code></pre> <p>In that above script, updating to use an ollama hosted mistral model (assuming that you have ollama installed and running) is as easy as updating the model to specify the ollama provider and using ollama model syntax for mistral!</p> <pre><code>model=\"ollama/mistral-small3.2:latest\"\n</code></pre>"},{"location":"quickstart/#streaming","title":"Streaming","text":"<p>For the providers that support streaming, you can enable it by passing <code>stream=True</code>:</p> <pre><code>output = \"\"\nfor chunk in completion(\n    model=\"mistral/mistral-small-latest\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n    stream=True\n):\n    chunk_content = chunk.choices[0].delta.content or \"\"\n    print(chunk_content)\n    output += chunk_content\n</code></pre>"},{"location":"quickstart/#embeddings","title":"Embeddings","text":"<p><code>embedding</code> and <code>aembedding</code> allow you to create vector embeddings from text using the same unified interface across providers.</p> <p>Not all providers support embeddings - check the providers documentation to see which ones do.</p> <pre><code>from any_llm import embedding\n\nresult = embedding(\n    model=\"openai/text-embedding-3-small\",\n    inputs=\"Hello, world!\" # can be either string or list of strings\n)\n\n# Access the embedding vector\nembedding_vector = result.data[0].embedding\nprint(f\"Embedding vector length: {len(embedding_vector)}\")\nprint(f\"Tokens used: {result.usage.total_tokens}\")\n</code></pre>"},{"location":"quickstart/#tools","title":"Tools","text":"<p><code>any-llm</code> supports tool calling for providers that support it. You can pass a list of tools where each tool is either:</p> <ol> <li>Python callable - Functions with proper docstrings and type annotations</li> <li>OpenAI Format tool dict - Already in OpenAI tool format</li> </ol> <pre><code>from any_llm import completion\n\ndef get_weather(location: str, unit: str = \"F\") -&gt; str:\n    \"\"\"Get weather information for a location.\n\n    Args:\n        location: The city or location to get weather for\n        unit: Temperature unit, either 'C' or 'F'\n    \"\"\"\n    return f\"Weather in {location} is sunny and 75{unit}!\"\n\nresponse = completion(\n    model=\"mistral/mistral-small-latest\",\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Pittsburgh PA?\"}],\n    tools=[get_weather]\n)\n</code></pre> <p>any-llm automatically converts your Python functions to OpenAI tools format. Functions must have: - A docstring describing what the function does - Type annotations for all parameters - A return type annotation</p>"},{"location":"api/completion/","title":"Completion","text":""},{"location":"api/completion/#completion","title":"Completion","text":""},{"location":"api/completion/#any_llm.completion","title":"<code>any_llm.completion(model, messages, *, tools=None, tool_choice=None, temperature=None, top_p=None, max_tokens=None, response_format=None, stream=None, n=None, stop=None, presence_penalty=None, frequency_penalty=None, seed=None, api_key=None, api_base=None, api_timeout=None, user=None, **kwargs)</code>","text":"<p>Create a chat completion.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier in format 'provider/model' (e.g., 'mistral/mistral-small')</p> required <code>messages</code> <code>list[dict[str, Any]]</code> <p>List of messages for the conversation</p> required <code>tools</code> <code>list[dict[str, Any] | Callable[..., Any]] | None</code> <p>List of tools for tool calling. Can be Python callables or OpenAI tool format dicts</p> <code>None</code> <code>tool_choice</code> <code>str | dict[str, Any] | None</code> <p>Controls which tools the model can call</p> <code>None</code> <code>temperature</code> <code>float | None</code> <p>Controls randomness in the response (0.0 to 2.0)</p> <code>None</code> <code>top_p</code> <code>float | None</code> <p>Controls diversity via nucleus sampling (0.0 to 1.0)</p> <code>None</code> <code>max_tokens</code> <code>int | None</code> <p>Maximum number of tokens to generate</p> <code>None</code> <code>response_format</code> <code>dict[str, Any] | type[BaseModel] | None</code> <p>Format specification for the response</p> <code>None</code> <code>stream</code> <code>bool | None</code> <p>Whether to stream the response</p> <code>None</code> <code>n</code> <code>int | None</code> <p>Number of completions to generate</p> <code>None</code> <code>stop</code> <code>str | list[str] | None</code> <p>Stop sequences for generation</p> <code>None</code> <code>presence_penalty</code> <code>float | None</code> <p>Penalize new tokens based on presence in text</p> <code>None</code> <code>frequency_penalty</code> <code>float | None</code> <p>Penalize new tokens based on frequency in text</p> <code>None</code> <code>seed</code> <code>int | None</code> <p>Random seed for reproducible results</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>api_timeout</code> <code>float | None</code> <p>Request timeout in seconds</p> <code>None</code> <code>user</code> <code>str | None</code> <p>Unique identifier for the end user</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>ChatCompletion | Iterator[ChatCompletionChunk]</code> <p>The completion response from the provider</p> Source code in <code>src/any_llm/api.py</code> <pre><code>def completion(\n    model: str,\n    messages: list[dict[str, Any]],\n    *,\n    tools: list[dict[str, Any] | Callable[..., Any]] | None = None,\n    tool_choice: str | dict[str, Any] | None = None,\n    temperature: float | None = None,\n    top_p: float | None = None,\n    max_tokens: int | None = None,\n    response_format: dict[str, Any] | type[BaseModel] | None = None,\n    stream: bool | None = None,\n    n: int | None = None,\n    stop: str | list[str] | None = None,\n    presence_penalty: float | None = None,\n    frequency_penalty: float | None = None,\n    seed: int | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    api_timeout: float | None = None,\n    user: str | None = None,\n    **kwargs: Any,\n) -&gt; ChatCompletion | Iterator[ChatCompletionChunk]:\n    \"\"\"Create a chat completion.\n\n    Args:\n        model: Model identifier in format 'provider/model' (e.g., 'mistral/mistral-small')\n        messages: List of messages for the conversation\n        tools: List of tools for tool calling. Can be Python callables or OpenAI tool format dicts\n        tool_choice: Controls which tools the model can call\n        temperature: Controls randomness in the response (0.0 to 2.0)\n        top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n        max_tokens: Maximum number of tokens to generate\n        response_format: Format specification for the response\n        stream: Whether to stream the response\n        n: Number of completions to generate\n        stop: Stop sequences for generation\n        presence_penalty: Penalize new tokens based on presence in text\n        frequency_penalty: Penalize new tokens based on frequency in text\n        seed: Random seed for reproducible results\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        api_timeout: Request timeout in seconds\n        user: Unique identifier for the end user\n        **kwargs: Additional provider-specific parameters\n\n    Returns:\n        The completion response from the provider\n\n    \"\"\"\n    provider, completion_params = _process_completion_params(\n        model=model,\n        messages=messages,\n        tools=tools,\n        tool_choice=tool_choice,\n        temperature=temperature,\n        top_p=top_p,\n        max_tokens=max_tokens,\n        response_format=response_format,\n        stream=stream,\n        n=n,\n        stop=stop,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        seed=seed,\n        api_key=api_key,\n        api_base=api_base,\n        api_timeout=api_timeout,\n        user=user,\n        **kwargs,\n    )\n\n    return provider.completion(completion_params, **kwargs)\n</code></pre>"},{"location":"api/completion/#any_llm.acompletion","title":"<code>any_llm.acompletion(model, messages, *, tools=None, tool_choice=None, temperature=None, top_p=None, max_tokens=None, response_format=None, stream=None, n=None, stop=None, presence_penalty=None, frequency_penalty=None, seed=None, api_key=None, api_base=None, api_timeout=None, user=None, **kwargs)</code>  <code>async</code>","text":"<p>Create a chat completion asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier in format 'provider/model' (e.g., 'mistral/mistral-small')</p> required <code>messages</code> <code>list[dict[str, Any]]</code> <p>List of messages for the conversation</p> required <code>tools</code> <code>list[dict[str, Any] | Callable[..., Any]] | None</code> <p>List of tools for tool calling. Can be Python callables or OpenAI tool format dicts</p> <code>None</code> <code>tool_choice</code> <code>str | dict[str, Any] | None</code> <p>Controls which tools the model can call</p> <code>None</code> <code>temperature</code> <code>float | None</code> <p>Controls randomness in the response (0.0 to 2.0)</p> <code>None</code> <code>top_p</code> <code>float | None</code> <p>Controls diversity via nucleus sampling (0.0 to 1.0)</p> <code>None</code> <code>max_tokens</code> <code>int | None</code> <p>Maximum number of tokens to generate</p> <code>None</code> <code>response_format</code> <code>dict[str, Any] | type[BaseModel] | None</code> <p>Format specification for the response</p> <code>None</code> <code>stream</code> <code>bool | None</code> <p>Whether to stream the response</p> <code>None</code> <code>n</code> <code>int | None</code> <p>Number of completions to generate</p> <code>None</code> <code>stop</code> <code>str | list[str] | None</code> <p>Stop sequences for generation</p> <code>None</code> <code>presence_penalty</code> <code>float | None</code> <p>Penalize new tokens based on presence in text</p> <code>None</code> <code>frequency_penalty</code> <code>float | None</code> <p>Penalize new tokens based on frequency in text</p> <code>None</code> <code>seed</code> <code>int | None</code> <p>Random seed for reproducible results</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>api_timeout</code> <code>float | None</code> <p>Request timeout in seconds</p> <code>None</code> <code>user</code> <code>str | None</code> <p>Unique identifier for the end user</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>ChatCompletion | Iterator[ChatCompletionChunk]</code> <p>The completion response from the provider</p> Source code in <code>src/any_llm/api.py</code> <pre><code>async def acompletion(\n    model: str,\n    messages: list[dict[str, Any]],\n    *,\n    tools: list[dict[str, Any] | Callable[..., Any]] | None = None,\n    tool_choice: str | dict[str, Any] | None = None,\n    temperature: float | None = None,\n    top_p: float | None = None,\n    max_tokens: int | None = None,\n    response_format: dict[str, Any] | type[BaseModel] | None = None,\n    stream: bool | None = None,\n    n: int | None = None,\n    stop: str | list[str] | None = None,\n    presence_penalty: float | None = None,\n    frequency_penalty: float | None = None,\n    seed: int | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    api_timeout: float | None = None,\n    user: str | None = None,\n    **kwargs: Any,\n) -&gt; ChatCompletion | Iterator[ChatCompletionChunk]:\n    \"\"\"Create a chat completion asynchronously.\n\n    Args:\n        model: Model identifier in format 'provider/model' (e.g., 'mistral/mistral-small')\n        messages: List of messages for the conversation\n        tools: List of tools for tool calling. Can be Python callables or OpenAI tool format dicts\n        tool_choice: Controls which tools the model can call\n        temperature: Controls randomness in the response (0.0 to 2.0)\n        top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n        max_tokens: Maximum number of tokens to generate\n        response_format: Format specification for the response\n        stream: Whether to stream the response\n        n: Number of completions to generate\n        stop: Stop sequences for generation\n        presence_penalty: Penalize new tokens based on presence in text\n        frequency_penalty: Penalize new tokens based on frequency in text\n        seed: Random seed for reproducible results\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        api_timeout: Request timeout in seconds\n        user: Unique identifier for the end user\n        **kwargs: Additional provider-specific parameters\n\n    Returns:\n        The completion response from the provider\n\n    \"\"\"\n    provider, completion_params = _process_completion_params(\n        model=model,\n        messages=messages,\n        tools=tools,\n        tool_choice=tool_choice,\n        temperature=temperature,\n        top_p=top_p,\n        max_tokens=max_tokens,\n        response_format=response_format,\n        stream=stream,\n        n=n,\n        stop=stop,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        seed=seed,\n        api_key=api_key,\n        api_base=api_base,\n        api_timeout=api_timeout,\n        user=user,\n        **kwargs,\n    )\n\n    return await provider.acompletion(completion_params, **kwargs)\n</code></pre>"},{"location":"api/embedding/","title":"Embedding","text":""},{"location":"api/embedding/#embedding","title":"Embedding","text":""},{"location":"api/embedding/#any_llm.embedding","title":"<code>any_llm.embedding(model, inputs, *, api_key=None, api_base=None, **kwargs)</code>","text":"<p>Create an embedding.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier in format 'provider/model' (e.g., 'mistral/mistral-small')</p> required <code>inputs</code> <code>str | list[str]</code> <p>The input text to embed</p> required <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>CreateEmbeddingResponse</code> <p>The embedding of the input text</p> Source code in <code>src/any_llm/api.py</code> <pre><code>def embedding(\n    model: str,\n    inputs: str | list[str],\n    *,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    **kwargs: Any,\n) -&gt; CreateEmbeddingResponse:\n    \"\"\"Create an embedding.\n\n    Args:\n        model: Model identifier in format 'provider/model' (e.g., 'mistral/mistral-small')\n        inputs: The input text to embed\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        **kwargs: Additional provider-specific parameters\n\n    Returns:\n        The embedding of the input text\n\n    \"\"\"\n    provider_key, model_name = ProviderFactory.split_model_provider(model)\n\n    config: dict[str, str] = {}\n    if api_key:\n        config[\"api_key\"] = str(api_key)\n    if api_base:\n        config[\"api_base\"] = str(api_base)\n    api_config = ApiConfig(**config)\n\n    provider = ProviderFactory.create_provider(provider_key, api_config)\n\n    return provider.embedding(model_name, inputs, **kwargs)\n</code></pre>"},{"location":"api/embedding/#any_llm.aembedding","title":"<code>any_llm.aembedding(model, inputs, *, api_key=None, api_base=None, **kwargs)</code>  <code>async</code>","text":"<p>Create an embedding asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier in format 'provider/model' (e.g., 'openai/text-embedding-3-small')</p> required <code>inputs</code> <code>str | list[str]</code> <p>The input text to embed</p> required <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>CreateEmbeddingResponse</code> <p>The embedding of the input text</p> Source code in <code>src/any_llm/api.py</code> <pre><code>async def aembedding(\n    model: str,\n    inputs: str | list[str],\n    *,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    **kwargs: Any,\n) -&gt; CreateEmbeddingResponse:\n    \"\"\"Create an embedding asynchronously.\n\n    Args:\n        model: Model identifier in format 'provider/model' (e.g., 'openai/text-embedding-3-small')\n        inputs: The input text to embed\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        **kwargs: Additional provider-specific parameters\n\n    Returns:\n        The embedding of the input text\n\n    \"\"\"\n    provider_key, model_name = ProviderFactory.split_model_provider(model)\n\n    config: dict[str, str] = {}\n    if api_key:\n        config[\"api_key\"] = str(api_key)\n    if api_base:\n        config[\"api_base\"] = str(api_base)\n    api_config = ApiConfig(**config)\n\n    provider = ProviderFactory.create_provider(provider_key, api_config)\n\n    return await provider.aembedding(model_name, inputs, **kwargs)\n</code></pre>"},{"location":"api/exceptions/","title":"Exceptions","text":""},{"location":"api/exceptions/#exceptions","title":"Exceptions","text":""},{"location":"api/exceptions/#any_llm.exceptions","title":"<code>any_llm.exceptions</code>","text":"<p>Custom exceptions for any-llm package.</p>"},{"location":"api/exceptions/#any_llm.exceptions.MissingApiKeyError","title":"<code>MissingApiKeyError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when an API key is missing or not provided.</p> Source code in <code>src/any_llm/exceptions.py</code> <pre><code>class MissingApiKeyError(Exception):\n    \"\"\"Exception raised when an API key is missing or not provided.\"\"\"\n\n    def __init__(self, provider_name: str, env_var_name: str) -&gt; None:\n        \"\"\"Initialize the exception.\n\n        Args:\n            provider_name: Name of the provider (e.g., \"OpenAI\", \"Google\", \"Mistral\")\n            env_var_name: Name of the environment variable that should contain the API key\n\n        \"\"\"\n        self.provider_name = provider_name\n        self.env_var_name = env_var_name\n\n        message = (\n            f\"No {provider_name} API key provided. \"\n            f\"Please provide it in the config or set the {env_var_name} environment variable.\"\n        )\n\n        super().__init__(message)\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.MissingApiKeyError.__init__","title":"<code>__init__(provider_name, env_var_name)</code>","text":"<p>Initialize the exception.</p> <p>Parameters:</p> Name Type Description Default <code>provider_name</code> <code>str</code> <p>Name of the provider (e.g., \"OpenAI\", \"Google\", \"Mistral\")</p> required <code>env_var_name</code> <code>str</code> <p>Name of the environment variable that should contain the API key</p> required Source code in <code>src/any_llm/exceptions.py</code> <pre><code>def __init__(self, provider_name: str, env_var_name: str) -&gt; None:\n    \"\"\"Initialize the exception.\n\n    Args:\n        provider_name: Name of the provider (e.g., \"OpenAI\", \"Google\", \"Mistral\")\n        env_var_name: Name of the environment variable that should contain the API key\n\n    \"\"\"\n    self.provider_name = provider_name\n    self.env_var_name = env_var_name\n\n    message = (\n        f\"No {provider_name} API key provided. \"\n        f\"Please provide it in the config or set the {env_var_name} environment variable.\"\n    )\n\n    super().__init__(message)\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.UnsupportedParameterError","title":"<code>UnsupportedParameterError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when an unsupported parameter is provided.</p> Source code in <code>src/any_llm/exceptions.py</code> <pre><code>class UnsupportedParameterError(Exception):\n    \"\"\"Exception raised when an unsupported parameter is provided.\"\"\"\n\n    def __init__(self, parameter_name: str, provider_name: str) -&gt; None:\n        \"\"\"Initialize the exception.\n\n        Args:\n            parameter_name: Name of the parameter that was provided\n            provider_name: Name of the provider that does not support the parameter\n\n        \"\"\"\n        self.parameter_name = parameter_name\n        self.provider_name = provider_name\n\n        message = f\"'{parameter_name}' is not supported for {provider_name}\"\n\n        super().__init__(message)\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.UnsupportedParameterError.__init__","title":"<code>__init__(parameter_name, provider_name)</code>","text":"<p>Initialize the exception.</p> <p>Parameters:</p> Name Type Description Default <code>parameter_name</code> <code>str</code> <p>Name of the parameter that was provided</p> required <code>provider_name</code> <code>str</code> <p>Name of the provider that does not support the parameter</p> required Source code in <code>src/any_llm/exceptions.py</code> <pre><code>def __init__(self, parameter_name: str, provider_name: str) -&gt; None:\n    \"\"\"Initialize the exception.\n\n    Args:\n        parameter_name: Name of the parameter that was provided\n        provider_name: Name of the provider that does not support the parameter\n\n    \"\"\"\n    self.parameter_name = parameter_name\n    self.provider_name = provider_name\n\n    message = f\"'{parameter_name}' is not supported for {provider_name}\"\n\n    super().__init__(message)\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.UnsupportedProviderError","title":"<code>UnsupportedProviderError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when an unsupported provider is requested.</p> Source code in <code>src/any_llm/exceptions.py</code> <pre><code>class UnsupportedProviderError(Exception):\n    \"\"\"Exception raised when an unsupported provider is requested.\"\"\"\n\n    def __init__(self, provider_key: str, supported_providers: list[str]) -&gt; None:\n        \"\"\"Initialize the exception.\n\n        Args:\n            provider_key: The provider key that was requested\n            supported_providers: List of supported provider keys\n\n        \"\"\"\n        self.provider_key = provider_key\n        self.supported_providers = supported_providers\n\n        message = f\"'{provider_key}' is not a supported provider. Supported providers: {', '.join(supported_providers)}\"\n\n        super().__init__(message)\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.UnsupportedProviderError.__init__","title":"<code>__init__(provider_key, supported_providers)</code>","text":"<p>Initialize the exception.</p> <p>Parameters:</p> Name Type Description Default <code>provider_key</code> <code>str</code> <p>The provider key that was requested</p> required <code>supported_providers</code> <code>list[str]</code> <p>List of supported provider keys</p> required Source code in <code>src/any_llm/exceptions.py</code> <pre><code>def __init__(self, provider_key: str, supported_providers: list[str]) -&gt; None:\n    \"\"\"Initialize the exception.\n\n    Args:\n        provider_key: The provider key that was requested\n        supported_providers: List of supported provider keys\n\n    \"\"\"\n    self.provider_key = provider_key\n    self.supported_providers = supported_providers\n\n    message = f\"'{provider_key}' is not a supported provider. Supported providers: {', '.join(supported_providers)}\"\n\n    super().__init__(message)\n</code></pre>"},{"location":"api/responses/","title":"Responses","text":""},{"location":"api/responses/#responses","title":"Responses","text":"<p>Warning</p> <p>This API is experimental and subject to changes based upon our experience as we integrate additional providers. Use with caution.</p>"},{"location":"api/responses/#any_llm.responses","title":"<code>any_llm.responses(model, input_data, *, tools=None, tool_choice=None, max_output_tokens=None, temperature=None, top_p=None, stream=None, api_key=None, api_base=None, api_timeout=None, user=None, instructions=None, max_tool_calls=None, parallel_tool_calls=None, reasoning=None, text=None, **kwargs)</code>","text":"<p>Create a response using the OpenAI-style Responses API.</p> <p>This follows the OpenAI Responses API shape and returns the aliased <code>any_llm.types.responses.Response</code> type. If <code>stream=True</code>, an iterator of <code>any_llm.types.responses.ResponseStreamEvent</code> items is returned.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier in format 'provider/model' (e.g., 'openai/gpt-4o')</p> required <code>input_data</code> <code>str | ResponseInputParam</code> <p>The input payload accepted by provider's Responses API. For OpenAI-compatible providers, this is typically a list mixing text, images, and tool instructions, or a dict per OpenAI spec.</p> required <code>tools</code> <code>list[dict[str, Any] | Callable[..., Any]] | None</code> <p>Optional tools for tool calling (Python callables or OpenAI tool dicts)</p> <code>None</code> <code>tool_choice</code> <code>str | dict[str, Any] | None</code> <p>Controls which tools the model can call</p> <code>None</code> <code>max_output_tokens</code> <code>int | None</code> <p>Maximum number of output tokens to generate</p> <code>None</code> <code>temperature</code> <code>float | None</code> <p>Controls randomness in the response (0.0 to 2.0)</p> <code>None</code> <code>top_p</code> <code>float | None</code> <p>Controls diversity via nucleus sampling (0.0 to 1.0)</p> <code>None</code> <code>stream</code> <code>bool | None</code> <p>Whether to stream response events</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>api_timeout</code> <code>float | None</code> <p>Request timeout in seconds</p> <code>None</code> <code>user</code> <code>str | None</code> <p>Unique identifier for the end user</p> <code>None</code> <code>instructions</code> <code>str | None</code> <p>A system (or developer) message inserted into the model's context.</p> <code>None</code> <code>max_tool_calls</code> <code>int | None</code> <p>The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.</p> <code>None</code> <code>parallel_tool_calls</code> <code>int | None</code> <p>Whether to allow the model to run tool calls in parallel.</p> <code>None</code> <code>reasoning</code> <code>Any | None</code> <p>Configuration options for reasoning models.</p> <code>None</code> <code>text</code> <code>Any | None</code> <p>Configuration options for a text response from the model. Can be plain text or structured JSON data.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Response | Iterator[ResponseStreamEvent]</code> <p>Either a <code>Response</code> object (non-streaming) or an iterator of</p> <code>Response | Iterator[ResponseStreamEvent]</code> <p><code>ResponseStreamEvent</code> (streaming).</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the selected provider does not support the Responses API.</p> Source code in <code>src/any_llm/api.py</code> <pre><code>def responses(\n    model: str,\n    input_data: str | ResponseInputParam,\n    *,\n    tools: list[dict[str, Any] | Callable[..., Any]] | None = None,\n    tool_choice: str | dict[str, Any] | None = None,\n    max_output_tokens: int | None = None,\n    temperature: float | None = None,\n    top_p: float | None = None,\n    stream: bool | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    api_timeout: float | None = None,\n    user: str | None = None,\n    instructions: str | None = None,\n    max_tool_calls: int | None = None,\n    parallel_tool_calls: int | None = None,\n    reasoning: Any | None = None,\n    text: Any | None = None,\n    **kwargs: Any,\n) -&gt; Response | Iterator[ResponseStreamEvent]:\n    \"\"\"Create a response using the OpenAI-style Responses API.\n\n    This follows the OpenAI Responses API shape and returns the aliased\n    `any_llm.types.responses.Response` type. If `stream=True`, an iterator of\n    `any_llm.types.responses.ResponseStreamEvent` items is returned.\n\n    Args:\n        model: Model identifier in format 'provider/model' (e.g., 'openai/gpt-4o')\n        input_data: The input payload accepted by provider's Responses API.\n            For OpenAI-compatible providers, this is typically a list mixing\n            text, images, and tool instructions, or a dict per OpenAI spec.\n        tools: Optional tools for tool calling (Python callables or OpenAI tool dicts)\n        tool_choice: Controls which tools the model can call\n        max_output_tokens: Maximum number of output tokens to generate\n        temperature: Controls randomness in the response (0.0 to 2.0)\n        top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n        stream: Whether to stream response events\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        api_timeout: Request timeout in seconds\n        user: Unique identifier for the end user\n        instructions: A system (or developer) message inserted into the model's context.\n        max_tool_calls: The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.\n        parallel_tool_calls: Whether to allow the model to run tool calls in parallel.\n        reasoning: Configuration options for reasoning models.\n        text: Configuration options for a text response from the model. Can be plain text or structured JSON data.\n\n        **kwargs: Additional provider-specific parameters\n\n    Returns:\n        Either a `Response` object (non-streaming) or an iterator of\n        `ResponseStreamEvent` (streaming).\n\n    Raises:\n        NotImplementedError: If the selected provider does not support the Responses API.\n\n    \"\"\"\n    provider_key, model_name = ProviderFactory.split_model_provider(model)\n\n    config: dict[str, str] = {}\n    if api_key:\n        config[\"api_key\"] = str(api_key)\n    if api_base:\n        config[\"api_base\"] = str(api_base)\n    api_config = ApiConfig(**config)\n\n    provider = ProviderFactory.create_provider(provider_key, api_config)\n\n    responses_kwargs = kwargs.copy()\n    if tools is not None:\n        responses_kwargs[\"tools\"] = prepare_tools(tools)\n    if tool_choice is not None:\n        responses_kwargs[\"tool_choice\"] = tool_choice\n    if max_output_tokens is not None:\n        responses_kwargs[\"max_output_tokens\"] = max_output_tokens\n    if temperature is not None:\n        responses_kwargs[\"temperature\"] = temperature\n    if top_p is not None:\n        responses_kwargs[\"top_p\"] = top_p\n    if stream is not None:\n        responses_kwargs[\"stream\"] = stream\n    if api_timeout is not None:\n        responses_kwargs[\"timeout\"] = api_timeout\n    if user is not None:\n        responses_kwargs[\"user\"] = user\n    if instructions is not None:\n        responses_kwargs[\"instructions\"] = instructions\n    if max_tool_calls is not None:\n        responses_kwargs[\"max_tool_calls\"] = max_tool_calls\n    if parallel_tool_calls is not None:\n        responses_kwargs[\"parallel_tool_calls\"] = parallel_tool_calls\n    if reasoning is not None:\n        responses_kwargs[\"reasoning\"] = reasoning\n    if text is not None:\n        responses_kwargs[\"text\"] = text\n\n    return provider.responses(model_name, input_data, **responses_kwargs)\n</code></pre>"},{"location":"api/responses/#any_llm.aresponses","title":"<code>any_llm.aresponses(model, input_data, *, tools=None, tool_choice=None, max_output_tokens=None, temperature=None, top_p=None, stream=None, api_key=None, api_base=None, api_timeout=None, user=None, instructions=None, max_tool_calls=None, parallel_tool_calls=None, reasoning=None, text=None, **kwargs)</code>  <code>async</code>","text":"<p>Create a response using the OpenAI-style Responses API.</p> <p>This follows the OpenAI Responses API shape and returns the aliased <code>any_llm.types.responses.Response</code> type. If <code>stream=True</code>, an iterator of <code>any_llm.types.responses.ResponseStreamEvent</code> items is returned.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier in format 'provider/model' (e.g., 'openai/gpt-4o')</p> required <code>input_data</code> <code>str | ResponseInputParam</code> <p>The input payload accepted by provider's Responses API. For OpenAI-compatible providers, this is typically a list mixing text, images, and tool instructions, or a dict per OpenAI spec.</p> required <code>tools</code> <code>list[dict[str, Any] | Callable[..., Any]] | None</code> <p>Optional tools for tool calling (Python callables or OpenAI tool dicts)</p> <code>None</code> <code>tool_choice</code> <code>str | dict[str, Any] | None</code> <p>Controls which tools the model can call</p> <code>None</code> <code>max_output_tokens</code> <code>int | None</code> <p>Maximum number of output tokens to generate</p> <code>None</code> <code>temperature</code> <code>float | None</code> <p>Controls randomness in the response (0.0 to 2.0)</p> <code>None</code> <code>top_p</code> <code>float | None</code> <p>Controls diversity via nucleus sampling (0.0 to 1.0)</p> <code>None</code> <code>stream</code> <code>bool | None</code> <p>Whether to stream response events</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>api_timeout</code> <code>float | None</code> <p>Request timeout in seconds</p> <code>None</code> <code>user</code> <code>str | None</code> <p>Unique identifier for the end user</p> <code>None</code> <code>instructions</code> <code>str | None</code> <p>A system (or developer) message inserted into the model's context.</p> <code>None</code> <code>max_tool_calls</code> <code>int | None</code> <p>The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.</p> <code>None</code> <code>parallel_tool_calls</code> <code>int | None</code> <p>Whether to allow the model to run tool calls in parallel.</p> <code>None</code> <code>reasoning</code> <code>Any | None</code> <p>Configuration options for reasoning models.</p> <code>None</code> <code>text</code> <code>Any | None</code> <p>Configuration options for a text response from the model. Can be plain text or structured JSON data.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Response | Iterator[ResponseStreamEvent]</code> <p>Either a <code>Response</code> object (non-streaming) or an iterator of</p> <code>Response | Iterator[ResponseStreamEvent]</code> <p><code>ResponseStreamEvent</code> (streaming).</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the selected provider does not support the Responses API.</p> Source code in <code>src/any_llm/api.py</code> <pre><code>async def aresponses(\n    model: str,\n    input_data: str | ResponseInputParam,\n    *,\n    tools: list[dict[str, Any] | Callable[..., Any]] | None = None,\n    tool_choice: str | dict[str, Any] | None = None,\n    max_output_tokens: int | None = None,\n    temperature: float | None = None,\n    top_p: float | None = None,\n    stream: bool | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    api_timeout: float | None = None,\n    user: str | None = None,\n    instructions: str | None = None,\n    max_tool_calls: int | None = None,\n    parallel_tool_calls: int | None = None,\n    reasoning: Any | None = None,\n    text: Any | None = None,\n    **kwargs: Any,\n) -&gt; Response | Iterator[ResponseStreamEvent]:\n    \"\"\"Create a response using the OpenAI-style Responses API.\n\n    This follows the OpenAI Responses API shape and returns the aliased\n    `any_llm.types.responses.Response` type. If `stream=True`, an iterator of\n    `any_llm.types.responses.ResponseStreamEvent` items is returned.\n\n    Args:\n        model: Model identifier in format 'provider/model' (e.g., 'openai/gpt-4o')\n        input_data: The input payload accepted by provider's Responses API.\n            For OpenAI-compatible providers, this is typically a list mixing\n            text, images, and tool instructions, or a dict per OpenAI spec.\n        tools: Optional tools for tool calling (Python callables or OpenAI tool dicts)\n        tool_choice: Controls which tools the model can call\n        max_output_tokens: Maximum number of output tokens to generate\n        temperature: Controls randomness in the response (0.0 to 2.0)\n        top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n        stream: Whether to stream response events\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        api_timeout: Request timeout in seconds\n        user: Unique identifier for the end user\n        instructions: A system (or developer) message inserted into the model's context.\n        max_tool_calls: The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.\n        parallel_tool_calls: Whether to allow the model to run tool calls in parallel.\n        reasoning: Configuration options for reasoning models.\n        text: Configuration options for a text response from the model. Can be plain text or structured JSON data.\n\n        **kwargs: Additional provider-specific parameters\n\n    Returns:\n        Either a `Response` object (non-streaming) or an iterator of\n        `ResponseStreamEvent` (streaming).\n\n    Raises:\n        NotImplementedError: If the selected provider does not support the Responses API.\n\n    \"\"\"\n    provider_key, model_name = ProviderFactory.split_model_provider(model)\n\n    config: dict[str, str] = {}\n    if api_key:\n        config[\"api_key\"] = str(api_key)\n    if api_base:\n        config[\"api_base\"] = str(api_base)\n    api_config = ApiConfig(**config)\n\n    provider = ProviderFactory.create_provider(provider_key, api_config)\n\n    responses_kwargs = kwargs.copy()\n    if tools is not None:\n        responses_kwargs[\"tools\"] = prepare_tools(tools)\n    if tool_choice is not None:\n        responses_kwargs[\"tool_choice\"] = tool_choice\n    if max_output_tokens is not None:\n        responses_kwargs[\"max_output_tokens\"] = max_output_tokens\n    if temperature is not None:\n        responses_kwargs[\"temperature\"] = temperature\n    if top_p is not None:\n        responses_kwargs[\"top_p\"] = top_p\n    if stream is not None:\n        responses_kwargs[\"stream\"] = stream\n    if api_timeout is not None:\n        responses_kwargs[\"timeout\"] = api_timeout\n    if user is not None:\n        responses_kwargs[\"user\"] = user\n    if instructions is not None:\n        responses_kwargs[\"instructions\"] = instructions\n    if max_tool_calls is not None:\n        responses_kwargs[\"max_tool_calls\"] = max_tool_calls\n    if parallel_tool_calls is not None:\n        responses_kwargs[\"parallel_tool_calls\"] = parallel_tool_calls\n    if reasoning is not None:\n        responses_kwargs[\"reasoning\"] = reasoning\n    if text is not None:\n        responses_kwargs[\"text\"] = text\n\n    return await provider.aresponses(model_name, input_data, **responses_kwargs)\n</code></pre>"}]}